# Vibe Coding Log

## Summary

During this session, I focused on continuing the TDD cycle using the qwen2.5-coder:32b model in
Ollama. However, due to hardware limitations, I had to switch to the 14b model. The primary
challenge was the slow response time with the 32b model, which was too large for my video card.
Switching to the 14b model provided a workable solution. I learned that local models on AMD are
slower, and larger models perform better, though models smaller than 8b vectors seem
counterproductive. This experience led me to adjust my workflow by opting for a smaller, yet
efficient model. Moving forward, I plan to test the performance of the qwen2.5-coder:32b model on a
Mac M2.

## Session Details

- **Date:** 04/20/2025
- **Start:** 08:56PM
- **End:** 10:18p
- **Duration:** 1h 22m

### Goals for the Session:

- Continued TDD cycle using the qwen2.5-coder:32b model running in Ollama
- Switched to 14b model due to hardware limitations.

### Tools Used:

- Avante
- qwen2.5-coder:32b
- qwen2.5-coder:14b
- qwen2.5-coder:8b

### Summary of Activities:

- Requested a list of unit test for the remove_files method.

### Challenges Encountered:

- Looks like the 32b model is to big to fit in my video card. Responses are to slow.

### Solutions or Workarounds:

- Switched to the 14b model.

### Key Learnings:

- Local models, on amd, are slower.
- Larger models do better. Duh... < 8b vectors seems counter productive.

### Impact on Workflow:

- I moved to a smaller model. but not to small.

### Next Steps:

- Try this on a mac m2 to see how that preforms with qwen2.5-coder:32b

