# Vibe Coding Log

## Summary

Overall, this was a beneficial session. Integrating AI into my workflow with the Avante plugin was
relatively straightforward. Although working with this new tool did slow me down, that is expected.
I think, as I get better at using these tools and understand the limits of the AI's capacity, I will
recover my lost productivity. 

This session was composed of the following activities.

1. I added appropriate files to the given context.
2. I asked the AI to analyze a specific, unimplemented method in the service layer.
3. I exchanged ideas about possible edge cases and the expected behavior.
4. I requested the AI to summarize the details of the discussed edge cases and added them to my todo
   list.
5. I requested the AI to generate unit tests for each test case and implement the solution in the
   service layer one test at a time for each test until complete.
6. I asked the AI to review the implementation and identify any additional failure points of the
   code.

## Session Details

- **Date:** 4/19/2025
- **Start:** 12:00p
- **End:** 1:35p
- **Duration:** 1h 35m

## Goals for the Session:
- Implement a service layer component using TDD.
- Test AI's ability to generate meaningful test cases and identify useful edge cases.

## Tools Used:
- GitHub's CoPilot
- Avante.vim plugin

## Summary of Activities:
- Ideation for edge cases that unit tests should cover
- Generation of unit tests
- Using pytest parameters, which I have not previously used
- Modification to existing implementation to make unit tests pass

## Challenges Encountered:
- The AI continually generated diffs that would place new test cases in the middle of existing
  tests.
- The Avante plugin has some annoying behaviors:
  - The text input box automatically enters "insert" mode.
  - The response text often exceeds the visible buffer and scrolls to the bottom automatically,
    requiring me to scroll back to the top with nearly every interaction.

## Solutions or Workarounds:
- I had to resort to copying the tests out and pasting them manually.

## Key Learnings:
- Create a coding standards document and keep it in the context of your chat the entire session.
- In cases where you are unfamiliar with a specific API, AI can provide very useful usage examples.
  This has extra value here because the examples exist within the context of your situation instead
  of an arbitrary widget factory example. This is not a substitute for reading the documentation.
- Small test generation is solid.
- Examples are key: "add a unit test and implement the fix to ensure names with leading and/or
  trailing whitespace are removed. " name\t" should be recorded as "name".

## Impact on Workflow:
- I was trying to be very precise with the AI and keep things "bite-sized" so I could easily validate
  its results. This slowed me down.
- Ideation for unit test edge cases was really good. The AI had suggestions that I might not
  otherwise have considered. I didn't add all the constraints it suggested, but it was good to consider
  them. This resulted in a better unit test suite.

## Next Steps:
- Next, we should try larger change sets. Instead of going one unit test at a time, allow the AI to
  generate multiple. Additionally, allow the AI to implement multiple solutions at a time.
- When we have more than one function implemented, I want to see how well the AI does at refactoring
  existing code to make it more concise (DRY, etc.).
- Consider generating prompts for common things. For example, I typed similar prompts several times
  to generate unit tests and implement code. This probably would impact speed significantly.

