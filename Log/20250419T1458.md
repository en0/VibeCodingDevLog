# Vibe Coding Log

## Summary

During this session, I focused on enhancing the TDD workflow by incorporating larger inputs and more
complex testing scenarios. The AI was tasked with generating multiple tests and corresponding code
solutions simultaneously. While the session was productive, I encountered challenges with large
requests that led to inefficient solutions. This highlighted the importance of maintaining smaller,
manageable contexts. Overall, the session reinforced the value of iterative development and the need
for precise communication with AI tools.

## Session Details

- **Date:** 04/19/2025
- **Start:** 02:58 PM
- **End:** 08:00 PM
- **Duration:** 3h 43m

### Goals for the Session:

- Continue TDD workflow but with larger inputs and more complex testing situations.
- Have the AI generate more than one test at a time.
- Have the AI generate code to solve multiple tests at once.

**NOTE**: I paused working at 3:50 and resumed at 6:15: 2h 25m

### Tools Used:

- GitLab Copilot
- Avante plugin

### Summary of Activities:

- Continued TDD approach to implement more service layer functions.
- Attempted larger requests to see how the AI would respond.
- Continued to lean on the AI for suggestions on what tests to use.
- We generated a lot of regex here and that was painless.

### Challenges Encountered:

- Large requests that resulted in "bad ideas" were hard to unwind. This negatively impacted my productivity.

### Solutions or Workarounds:

- Use smaller contexts

### Key Learnings:

- Having all the files added to the context is really, really important. This includes things like mocks and fixtures that you have already created as well as interfaces with good docstrings for service-level dependencies so the AI can see what exceptions might be expected as well as the limit of responsibilities for various objects.
- Again, examples really help. The initial test generation was terrible. It wasn't using the correct mocks and was reaching inside private members of the unit to get at dependencies.
- The longer the session goes on, the better the AI is at following the pre-existing patterns.
- Very large contexts, say 3 or more tests + implementations, are hard to deal with. Not only does the AI make mistakes, it's harder as a human to review all the changes. The better you can describe what you want, the more accurate it is. There is a cost/benefit tipping point somewhere here where the size of the thing you want the AI to do would take longer to describe than breaking it into smaller pieces and iterating. I think this is because you can grant the AI some creative license, but if you give it too much rope, it deviates too far from the desired path. Lesson: prefer smaller iterations over big changes.
- Reuse your prompts. You really only need a few of them for most of the work. Ad hoc prompts are few and far between.

### Impact on Workflow:

- I was generally better at this cycle the second time through. I believe continued practice will improve performance even more.
- Large requests cost me more time in reviewing the code and fixing issues with it. The "bad ideas" in the resulting code propagated through multiple pages and took more effort to identify as a bad idea and to correct than if I had aimed for smaller change sets with more iterations.

### Next Steps:

- Refine the context size for each request. How big is too big?
- Make sure you are including the correct files with each request and remind the AI to follow standards.
