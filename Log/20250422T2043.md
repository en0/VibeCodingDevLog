# Vibe Coding Log

## Summary

In this session, the focus was on streamlining the workflow by implementing the remaining functional
tests for the `remove_files` method. The AI was utilized to analyze the code and identify additional
edge cases, stubbing out tests for implementation. Only one test required additional code,
indicating potential issues with its suitability for measuring cycle time. Challenges included the
AI's verbose responses, which were mitigated by instructing it to limit responses to necessary
changes. Key learnings highlighted the benefits of minimal responses, reusing prompts, and creating
repeatable prompts. The session positively impacted the workflow, and future steps include speeding
up the TDD cycle and testing service code refactoring.

## Session Details

- **Date:** 04/22/2025
- **Start:** 08:43 PM
- **End:** 09:44 AM
- **Duration:** 1 hours 1 minutes

### Goals for the Session:

- Streamline my workflow and see how much I can accomplish in the shortest amount of time.

### Tools Used:

- Avante
- Github Copilot

### Summary of Activities:

In this session, we implemented the remaining functional tests for `remove_files`. We then had the
AI analyze the code and identify additional edge cases. The AI stubbed out these tests without
implementation and was asked to implement them one at a time.

Only one test required additional code to pass. This suggests that this test may not be suitable for
measuring the full cycle time.

### Challenges Encountered:

- As usual, the AI provides detailed responses with extensive explanations. These changes are
  relatively simple, making it time-consuming to read through the entire response.

### Solutions or Workarounds:

- Prepend your requests with: "Limit your response to only the required changes."

### Key Learnings:

- Instruct the AI to be less verbose. See workarounds above.
- Requesting minimal responses had a massive beneficial effect.
- Reusing prompts and creating repeatable prompts is a significant time-saver.
- You can replay prompts in Avante by going into the chat log and pressing `r` on the last prompt.
- Stubbing out unit tests like this helps the AI generate tests according to my preferences.

```python
def test_remove_files_case_sensitivity(unit: ConfigSetService):
    # given: a configuration set with some files
    # when: file names with different cases are passed to remove_files
    # then: ensure consistent case sensitivity
    assert True
```

### Impact on Workflow:

- Minimal response: Positive
- Reused prompts: Positive
- Larger models generally perform better.

### Next Steps:

- Continue to speed up the TDD cycle. Need a better test on a new method.
- Test out refactoring the service code after multiple methods are implemented.

### Questions

- Is it possible to set prompt snippets that are always included? For example, "Limit your response
  to only the required changes" and "Follow the coding standards listed in the standards.md
  document."
